{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc7b3cb4-9b4e-408e-8a73-ed3ea15cc415",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c21d8179-3660-4418-a6fe-1c4c4ae46e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de0789-8efb-42f3-86e9-b9a277c3b343",
   "metadata": {},
   "source": [
    "## Define the Sparse Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff45048e-e427-4945-827f-92c5ba2f0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sparsity_lambda=1e-3, sparsity_target=0.05):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sparsity_lambda = sparsity_lambda\n",
    "        self.sparsity_target = sparsity_target\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = F.relu(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "    def sparsity_loss(self, encoded):\n",
    "        # Calculate the mean activation per neuron in the hidden layer\n",
    "        mean_activation = torch.mean(encoded, dim=0)\n",
    "        \n",
    "        # KL divergence between target sparsity and actual sparsity\n",
    "        kl_divergence = self.sparsity_target * torch.log(self.sparsity_target / mean_activation) + \\\n",
    "                        (1 - self.sparsity_target) * torch.log((1 - self.sparsity_target) / (1 - mean_activation))\n",
    "        \n",
    "        return self.sparsity_lambda * torch.sum(kl_divergence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc00d69-66b3-44d6-aab7-445262b52ce0",
   "metadata": {},
   "source": [
    "## Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "327c03f2-8197-4c47-903e-084497f9522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (111, 35)\n",
      "y_train shape: (111, 1)\n",
      "X_test shape: (54, 35)\n"
     ]
    }
   ],
   "source": [
    "# Define the path where the data is located\n",
    "data_path = r\"C:/Users/divyas/Documents/hackathons/CMI_PB/Tasks/Task_IGg_PT/Data_Task_IGg_PT/\"\n",
    "\n",
    "# Load the datasets using the data path\n",
    "X_train = pd.read_csv(data_path + \"abtiter_data_X_train.csv\", index_col=0)\n",
    "y_train = pd.read_csv(data_path + \"abtiter_data_y_train.csv\", index_col=0)\n",
    "X_test = pd.read_csv(data_path + \"abtiter_data_X_test.csv\", index_col=0)\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70e86773-01c6-45b5-9955-8353a986f70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             IgG_PRN    IgG_FHA   IgG1_PT  IgG1_PRN  IgG1_FHA  IgG1_FIM2/3  \\\n",
      "subject_id                                                                   \n",
      "1           2.602350  34.050956  7.334714  2.174783  3.013252     1.188744   \n",
      "3           7.652635   1.096457  1.424098  3.161591  1.287515     0.322658   \n",
      "4           5.670403   1.048276  3.888604  2.591155  1.269821     2.621216   \n",
      "5           5.268274   0.084437  7.456313  2.760065  2.864834     7.487345   \n",
      "6           0.090176   0.379290  0.084132  0.025479  0.654192     0.681225   \n",
      "\n",
      "             IgG1_TT   IgG1_DT   IgG1_OVA   IgG2_PT  ...   IgG3_OVA  \\\n",
      "subject_id                                           ...              \n",
      "1           1.428852  2.389153   0.665203  1.000000  ...   1.865388   \n",
      "3           1.377390  1.523941  33.771912  1.000000  ...   1.119233   \n",
      "4           1.675259  2.022924   5.777047  4.269877  ...   1.000000   \n",
      "5           1.537432  2.250237   4.130732  6.070427  ...   5.446934   \n",
      "6           0.874920  0.369367  10.452881  1.000000  ...  13.206949   \n",
      "\n",
      "              IgG4_PT   IgG4_PRN   IgG4_FHA  IgG4_FIM2/3   IgG4_TT   IgG4_DT  \\\n",
      "subject_id                                                                     \n",
      "1            1.061706  11.673594   0.880611     0.980223  3.290050  1.232849   \n",
      "3            1.000000   0.733287   0.057114     0.980223  0.024820  0.003253   \n",
      "4            6.582579   3.261863   1.089128    40.303354  1.635454  0.634256   \n",
      "5           44.804003   1.112574  24.353645     4.855024  0.920018  1.879391   \n",
      "6            1.000000   0.208993   0.984870     1.529665  3.565218  0.676574   \n",
      "\n",
      "            IgG4_OVA  infancy_vac  biological_sex  \n",
      "subject_id                                         \n",
      "1           2.622675            0               0  \n",
      "3           0.053981            0               0  \n",
      "4           2.021985            0               1  \n",
      "5           1.569320            0               1  \n",
      "6           7.648106            0               0  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "             IgG_PRN   IgG_FHA    IgG1_PT  IgG1_PRN  IgG1_FHA  IgG1_FIM2/3  \\\n",
      "subject_id                                                                   \n",
      "142         0.066482  0.102288   0.452381  1.343446  0.637104     1.250859   \n",
      "146         3.039959  2.687800  27.488095  3.248061  2.481587     0.508973   \n",
      "163         0.052871  0.002726   0.510791  0.025011  0.010421     0.008366   \n",
      "124         1.053969  0.299007   1.416667  0.911267  0.298782     0.068729   \n",
      "134         1.344032  0.987074   0.904762  1.208517  1.225409     1.037801   \n",
      "\n",
      "             IgG1_TT   IgG1_DT  IgG1_OVA   IgG2_PT  ...  IgG3_OVA  IgG4_PT  \\\n",
      "subject_id                                          ...                      \n",
      "142         0.137712  0.252301  0.215054  0.419355  ...  0.991379  0.43750   \n",
      "146         0.056218  0.830831  0.309677  0.709677  ...  0.724138  0.65625   \n",
      "163         0.291480  0.334586  0.395062  0.666667  ...  1.615385  0.56250   \n",
      "124         0.525460  1.147328  0.572043  0.870968  ...  0.672414  1.40625   \n",
      "134         2.272647  1.163639  1.668817  1.032258  ...  0.637931  0.93750   \n",
      "\n",
      "             IgG4_PRN  IgG4_FHA  IgG4_FIM2/3   IgG4_TT   IgG4_DT  IgG4_OVA  \\\n",
      "subject_id                                                                   \n",
      "142          0.454545  0.275735     0.630000  0.049354  0.388167  0.050901   \n",
      "146          1.636364  0.107843     0.800000  0.056404  3.195280  0.027928   \n",
      "163          0.140719  0.143443     0.472727  0.257426  0.079650  0.295082   \n",
      "124         16.083333  3.752451     1.040000  8.465335  8.604651  0.111712   \n",
      "134          0.833333  0.276961     0.840000  0.291422  0.094391  2.032207   \n",
      "\n",
      "            infancy_vac  biological_sex  \n",
      "subject_id                               \n",
      "142                   1               0  \n",
      "146                   0               1  \n",
      "163                   0               0  \n",
      "124                   1               1  \n",
      "134                   0               1  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Mapping categorical columns to numerical values\n",
    "X_train['infancy_vac'] = X_train['infancy_vac'].map({'wP': 0, 'aP': 1})\n",
    "X_train['biological_sex'] = X_train['biological_sex'].map({'Female': 0, 'Male': 1})\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "X_train = X_train.drop(columns=['dataset', 'timepoint', 'date_of_boost'])\n",
    "\n",
    "# Display the modified dataframe\n",
    "print(X_train.head())\n",
    "\n",
    "X_test['infancy_vac'] = X_test['infancy_vac'].map({'wP': 0, 'aP': 1})\n",
    "X_test['biological_sex'] = X_test['biological_sex'].map({'Female': 0, 'Male': 1})\n",
    "\n",
    "# Dropping the unnecessary columns\n",
    "X_test = X_test.drop(columns=['dataset', 'timepoint', 'date_of_boost'])\n",
    "\n",
    "# Display the modified dataframe\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5df54297-76ac-45b4-8402-f7aedd5a3936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs batch shape: torch.Size([32, 32]), Labels batch shape: torch.Size([32, 1])\n",
      "Inputs batch shape: torch.Size([32, 32]), Labels batch shape: torch.Size([32, 1])\n",
      "Inputs batch shape: torch.Size([32, 32]), Labels batch shape: torch.Size([32, 1])\n",
      "Inputs batch shape: torch.Size([15, 32]), Labels batch shape: torch.Size([15, 1])\n"
     ]
    }
   ],
   "source": [
    "# Standardize the dataset (fit only on training data)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert the standardized data into tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# You can now use train_loader for training your model\n",
    "# For testing, just create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(X_test_tensor)  # y_test is not available for challenge set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "for inputs, labels in train_loader:\n",
    "    # inputs and labels are your batches\n",
    "    print(f'Inputs batch shape: {inputs.shape}, Labels batch shape: {labels.shape}')\n",
    "    # You can now pass inputs and labels to your model for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d2340-5597-4aa2-b5cf-e6115539e290",
   "metadata": {},
   "source": [
    "## Training the Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac43a484-02ee-4dea-b7d0-b96bd7e1cf6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m  batch_data \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract tensor if batch is a tuple/list\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Move to the correct device if necessary (e.g., for GPU support)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m  batch_data \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[0;32m     20\u001b[0m  optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     22\u001b[0m  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Define model, optimizer, and loss function\n",
    "input_dim = X_train_scaled.shape[1]  # Input dimension based on X_train\n",
    "hidden_dim = 64  # Adjust hidden dimensions based on your data\n",
    "\n",
    "model = SparseAutoencoder(input_dim=input_dim, hidden_dim=hidden_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 50\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch_data in train_loader:  # Remove the extra comma here\n",
    "\n",
    "        # Unpack batch data correctly\n",
    "        batch_data = batch[0]  # Extract tensor if batch is a tuple/list\n",
    "    \n",
    "       # Move to the correct device if necessary (e.g., for GPU support)\n",
    "        # batch_data = batch_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed, encoded = model(batch_data)\n",
    "        \n",
    "        # Compute reconstruction loss\n",
    "        reconstruction_loss = F.mse_loss(reconstructed, batch_data)\n",
    "        \n",
    "        # Compute sparsity loss\n",
    "        sparsity_loss = model.sparsity_loss(encoded)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + sparsity_loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124e25e8-60ff-4610-9a7d-9c7d072d236a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd6d487-fc55-4231-929d-1847f4093aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
